{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ensembling.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qxTr-XiPf8aY"
      },
      "outputs": [],
      "source": [
        "# importing utility modules\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        " \n",
        "# importing machine learning models for prediction\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "import xgboost as xgb\n",
        "from sklearn.linear_model import LinearRegression\n",
        " \n",
        "# importing stacking lib\n",
        "from vecstack import stacking\n",
        "\n",
        "# loading train data set in dataframe from train_data.csv file\n",
        "df = pd.read_csv(\"train_data.csv\")\n",
        " \n",
        "# getting target data from the dataframe\n",
        "target = df[\"target\"]\n",
        " \n",
        "# getting train data from the dataframe\n",
        "train = df.drop(\"target\")\n",
        " \n",
        "# Splitting between train data into training and validation dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    train, target, test_size=0.20)\n",
        " \n",
        " \n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stacking"
      ],
      "metadata": {
        "id": "lBhRtVJHg2tT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# initializing all the base model objects with default parameters\n",
        "model_1 = LinearRegression()\n",
        "model_2 = xgb.XGBRegressor()\n",
        "model_3 = RandomForestRegressor()\n",
        " \n",
        "# putting all base model objects in one list\n",
        "all_models = [model_1, model_2, model_3]\n",
        " \n",
        "# computing the stack features\n",
        "s_train, s_test = stacking(all_models, X_train, X_test,\n",
        "                           y_train, regression=True, n_folds=4)\n",
        " \n",
        "# initializing the second-level model\n",
        "final_model = model_1\n",
        " \n",
        "# fitting the second level model with stack features\n",
        "final_model = final_model.fit(s_train, y_train)\n",
        " \n",
        "# predicting the final output using stacking\n",
        "pred_final = final_model.predict(X_test)\n",
        " \n",
        "# printing the root mean squared error between real value and predicted value\n",
        "print(mean_squared_error(y_test, pred_final))"
      ],
      "metadata": {
        "id": "02Q7QaXrgMdR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bagging"
      ],
      "metadata": {
        "id": "gZI8bHv4hJZu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# importing utility modules\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        " \n",
        "# importing machine learning models for prediction\n",
        "import xgboost as xgb\n",
        " \n",
        "# importing bagging module\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        " \n",
        "# loading train data set in dataframe from train_data.csv file\n",
        "df = pd.read_csv(\"train_data.csv\")\n",
        " \n",
        "# getting target data from the dataframe\n",
        "target = df[\"target\"]\n",
        " \n",
        "# getting train data from the dataframe\n",
        "train = df.drop(\"target\")\n",
        " \n",
        "# Splitting between train data into training and validation dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    train, target, test_size=0.20)\n",
        " \n",
        "# initializing the bagging model using XGboost as base model with default parameters\n",
        "model = BaggingRegressor(base_estimator=xgb.XGBRegressor())\n",
        " \n",
        "# training model\n",
        "model.fit(X_train, y_train)\n",
        " \n",
        "# predicting the output on the test dataset\n",
        "pred = model.predict(X_test)\n",
        " \n",
        "# printing the root mean squared error between real value and predicted value\n",
        "print(mean_squared_error(y_test, pred_final))"
      ],
      "metadata": {
        "id": "QYToXirDhKc0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Boosting"
      ],
      "metadata": {
        "id": "jrgj1oDphPay"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# importing utility modules\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        " \n",
        "# importing machine learning models for prediction\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        " \n",
        "# loading train data set in dataframe from train_data.csv file\n",
        "df = pd.read_csv(\"train_data.csv\")\n",
        " \n",
        "# getting target data from the dataframe\n",
        "target = df[\"target\"]\n",
        " \n",
        "# getting train data from the dataframe\n",
        "train = df.drop(\"target\")\n",
        " \n",
        "# Splitting between train data into training and validation dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    train, target, test_size=0.20)\n",
        " \n",
        "# initializing the boosting module with default parameters\n",
        "model = GradientBoostingRegressor()\n",
        " \n",
        "# training the model on the train dataset\n",
        "model.fit(X_train, y_train)\n",
        " \n",
        "# predicting the output on the test dataset\n",
        "pred_final = model.predict(X_test)\n",
        " \n",
        "# printing the root mean squared error between real value and predicted value\n",
        "print(mean_squared_error(y_test, pred_final))"
      ],
      "metadata": {
        "id": "ldF_s1ryhSGl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stacking using NN"
      ],
      "metadata": {
        "id": "JnzZHVnnj784"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Neural networks\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Add, Input, Dense, Dropout\n",
        "from tensorflow.keras.layers import BatchNormalization, Embedding\n",
        "from tensorflow.keras.layers import Flatten, Concatenate\n",
        "from tensorflow.keras import regularizers\n",
        "from keras.regularizers import l1\n",
        "from keras.regularizers import l2\n",
        "from tensorflow.keras import regularizers\n",
        "# Wrapper to make neural network compitable with StackingRegressor\n",
        "from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n",
        "# Linear model as meta-learn\n",
        "from sklearn.linear_model import LinearRegression\n",
        "# Create generic dataset for regression\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Create regression dataset\n",
        "X, y = make_regression(n_targets=1, random_state=42)\n",
        "# Convert to pandas\n",
        "X = pd.DataFrame(X)\n",
        "y = pd.DataFrame(y)\n",
        "#Rename column\n",
        "y = y.rename(columns={0: 'target'})\n",
        "# Split into validation set\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y,\n",
        "                                                  test_size=0.2,\n",
        "                                                  random_state=42)"
      ],
      "metadata": {
        "id": "lMFSuxSuj_4m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_neural_network(input_shape=510, depth=10, batch_mod=2, num_neurons=250, drop_rate=0.1, learn_rate=.001,\n",
        "                      r1_weight=0.02,\n",
        "                      r2_weight=0.02):\n",
        "    '''A neural network architecture built using keras functional API'''\n",
        "    act_reg = l1(r2_weight)\n",
        "    kern_reg = l1(r1_weight)\n",
        "    \n",
        "    inputs = Input(shape=(input_shape,))\n",
        "batch1 = BatchNormalization()(inputs)\n",
        "hidden1 = Dense(num_neurons, activation='relu', kernel_regularizer=kern_reg, activity_regularizer=act_reg)(batch1)\n",
        "    dropout1 = Dropout(drop_rate)(hidden1)\n",
        "    hidden2 = Dense(int(num_neurons/2), activation='relu', kernel_regularizer=kern_reg, activity_regularizer=act_reg)(dropout1)\n",
        "    \n",
        "    skip_list = [batch1]\n",
        "    last_layer_in_loop = hidden2\n",
        "    \n",
        "    for i in range(depth):\n",
        "        added_layer = concatenate(skip_list + [last_layer_in_loop])\n",
        "        skip_list.append(added_layer)\n",
        "b1 = None\n",
        "        #Apply batch only on every i % N layers\n",
        "        if i % batch_mod == 2:\n",
        "            b1 = BatchNormalization()(added_layer)\n",
        "        else:\n",
        "            b1 = added_layer\n",
        "        \n",
        "        h1 = Dense(num_neurons, activation='relu', kernel_regularizer=kern_reg, activity_regularizer=act_reg)(b1)\n",
        "        d1 = Dropout(drop_rate)(h1)\n",
        "        h2 = Dense(int(num_neurons/2), activation='relu', kernel_regularizer=kern_reg, activity_regularizer=act_reg)(d1)\n",
        "        d2 = Dropout(drop_rate)(h2)\n",
        "        h3 =  Dense(int(num_neurons/2), activation='relu', kernel_regularizer=kern_reg, activity_regularizer=act_reg)(d2)\n",
        "        d3 = Dropout(drop_rate)(h3)\n",
        "        h4 =  Dense(int(num_neurons/2), activation='relu', kernel_regularizer=kern_reg, activity_regularizer=act_reg)(d3)\n",
        "        \n",
        "last_layer_in_loop = h4\n",
        "\n",
        "c1 = concatenate(skip_list + [last_layer_in_loop])\n",
        "    output = Dense(1, activation='sigmoid')(c1)\n",
        "    \n",
        "    model = Model(inputs=inputs, outputs=output)\n",
        "\n",
        "optimizer = Adam()\n",
        "    optimizer.learning_rate = learn_rate\n",
        "    \n",
        "    model.compile(optimizer=optimizer,\n",
        "                  loss='mse',\n",
        "                  metrics=['accuracy'])\n",
        "return model"
      ],
      "metadata": {
        "id": "8zXPcTuJk1__"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_stacking(input_shape=None):\n",
        "    '''A stacking model that consists of CatBoostRegressor,\n",
        "    XGBRegressor, a linear model, and some neural networks'''\n",
        "    # First we create a list called \"level0\", which consists of our base models\"\n",
        "    # These models will get passed down to the meta-learner later\n",
        "    level0 = list()\n",
        "level0.append(('cat', CatBoostRegressor(verbose=False)))\n",
        "    level0.append(('cat2', CatBoostRegressor(verbose=False, learning_rate=.0001)))\n",
        "    level0.append(('xgb', XGBRegressor()))\n",
        "    level0.append(('xgb2', XGBRegressor(max_depth=5, learning_rate=.0001)))\n",
        "    level0.append(('linear', LinearRegression()))\n",
        "#Create 5 neural networks using our function above\n",
        "    for i in range(5):\n",
        "        # Wrap our neural network in a Keras Regressor to make it\n",
        "        #compatible with StackingRegressor\n",
        "        keras_reg = KerasRegressor(\n",
        "                create_neural_network, # Pass in function\n",
        "                input_shape=input_shape, # Pass in the dimensions to above function\n",
        "                epochs=6,\n",
        "                batch_size=32,\n",
        "                verbose=False)\n",
        "        keras_reg._estimator_type = \"regressor\"\n",
        "        # Append to our list\n",
        "        level0.append(('nn_{num}'.format(num=i), keras_reg))\n",
        "# The \"meta-learner\" designated as the level1 model\n",
        "    # In my experience Linear Regression performs best\n",
        "    # but feel free to experiment with other models\n",
        "    level1 = LinearRegression()\n",
        "# Create the stacking ensemble\n",
        "    model = StackingRegressor(estimators=level0, final_estimator=level1, cv=2, verbose=1)\n",
        "    return model"
      ],
      "metadata": {
        "id": "jjpaN3wukam3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Get our input dimensions for neural network\n",
        "input_dimensions = len(X_train.columns)\n",
        "# Create stacking model\n",
        "model = get_stacking(input_dimensions)\n",
        "model.fit(X_train, y_train.values.ravel())\n",
        "# Creating a temporary dataframe so we can see how each of our models performed\n",
        "temp = pd.DataFrame(y_val)\n",
        "# The stacked models predictions, which should perform the best\n",
        "temp['stacking_prediction'] = model.predict(X_val)\n",
        "# Get each model in the stacked model to see how they individually perform\n",
        "for m in model.named_estimators_:\n",
        "        temp[m] = model.named_estimators_[m].predict(X_val)\n",
        "# See how each of our models correlate with our target\n",
        "print(temp.corr()['target'])\n",
        "# See what our meta-learner is thinking (the linear regression)\n",
        "for coef in zip(model.named_estimators_, model.final_estimator_.coef_):\n",
        "    print(coef)"
      ],
      "metadata": {
        "id": "6W1_9ZQxkdle"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}